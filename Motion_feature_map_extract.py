import osimport subprocessimport numpy as npimport torchimport torch.nn as nn# Extract motion vectors and save processed feature maps# Before running this, make sure you have installed this extractor https://github.com/LukasBommes/mv-extractordef process_videos(input_video_folder, output_folder):    for i in range(1, 44):        video_file = f"video_{i}.mp4"        input_video_path = os.path.join(input_video_folder, video_file)        if os.path.exists(input_video_path):            print(f"Processing {video_file}...")            output_path = os.path.join(output_folder, f"vid_{i}_mvs_dump")            command = ['extract_mvs', input_video_path, '--dump', output_path]            subprocess.run(command)        else:            print(f"Video file {video_file} does not exist, skipping.")# Generate motion feature mapsdef motion_feature_map(data, resolution, block_size=(4, 4)):    height, width = resolution    block_h, block_w = block_size    feature_map = np.zeros((height // block_h, width // block_w), dtype=np.float32)    if data.shape[0] == 0:        return feature_map    for row in data:        dst_x, dst_y = row[5], row[6]        motion_x, motion_y, motion_scale = row[7], row[8], row[9]        motion_x /= motion_scale        motion_y /= motion_scale        magnitude = np.sqrt(motion_x ** 2 + motion_y ** 2)        block_row, block_col = int(dst_y // block_h), int(dst_x // block_w)        if 0 <= block_row < feature_map.shape[0] and 0 <= block_col < feature_map.shape[1]:            feature_map[block_row, block_col] += magnitude    feature_map /= np.max(feature_map) if np.max(feature_map) > 0 else 1.0    return feature_map# Save feature maps from motion vectorsdef save_feature_map(root_dir, video_range, resolution, block_size=(4, 4)):    for video_index in video_range:        video_dir = os.path.join(root_dir, f"vid_{video_index}_mvs_dump", "motion_vectors")        if not os.path.isdir(video_dir):            print(f"Directory not found: {video_dir}")            continue        output_dir = os.path.join(root_dir, f"vid_{video_index}_processed")        os.makedirs(output_dir, exist_ok=True)        for frame_file in os.listdir(video_dir):            if frame_file.endswith(".npy"):                data = np.load(os.path.join(video_dir, frame_file))                feature_map = motion_feature_map(data, resolution, block_size)                output_file = os.path.join(output_dir, f"processed_{frame_file}")                np.save(output_file, feature_map)                print(f"Saved: {output_file}")# Combine feature maps into a single video matrixdef save_combined_video_matrix(root_dir, video_range, output_file):    combined_matrices = []    for video_index in video_range:        processed_dir = os.path.join(root_dir, f"vid_{video_index}_processed")        if not os.path.isdir(processed_dir):            print(f"Processed directory not found: {processed_dir}")            continue        video_matrix = []        for frame_file in sorted(os.listdir(processed_dir)):            if frame_file.endswith(".npy"):                frame_path = os.path.join(processed_dir, frame_file)                feature_map = np.load(frame_path)                video_matrix.append(feature_map)        if video_matrix:            video_matrix = np.stack(video_matrix, axis=0)            combined_matrices.append(video_matrix)    if combined_matrices:        combined_matrices = np.stack(combined_matrices, axis=0)        np.save(output_file, combined_matrices)        print(f"Saved combined video matrix to {output_file}")# Define CNN for feature extraction in ILCASclass CNNSubNetwork(nn.Module):    def __init__(self):        super().__init__()        self.layers = nn.Sequential(            nn.Conv2d(1, 32, 5, padding=2), nn.ReLU(), nn.MaxPool2d(3, 3),            nn.Conv2d(32, 32, 5, padding=2), nn.ReLU(), nn.MaxPool2d(3, 3),            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2)        )    def forward(self, x):        return self.layers(x)# Extract features from a single video matrixdef extract_features_from_video(video_matrix_path, model, output_dir):    video_matrix = np.load(video_matrix_path)    video_tensor = torch.tensor(video_matrix, dtype=torch.float32).unsqueeze(1)    with torch.no_grad():        model.eval()        features = model(video_tensor)        flattened_features = features.view(features.size(0), -1).numpy()    output_path = os.path.join(output_dir, os.path.basename(video_matrix_path).replace("_matrix.npy", "_features.npy"))    np.save(output_path, flattened_features)    print(f"Saved features to {output_path}")# Batch process feature extractiondef batch_extract_features(root_dir, video_range, model, output_root_dir):    for video_index in video_range:        processed_dir = os.path.join(root_dir, f"vid_{video_index}_processed")        video_matrix_path = os.path.join(processed_dir, f"video_{video_index}_matrix.npy")        if os.path.exists(video_matrix_path):            output_dir = os.path.join(output_root_dir, f"vid_{video_index}_features")            os.makedirs(output_dir, exist_ok=True)            extract_features_from_video(video_matrix_path, model, output_dir)        else:            print(f"Video matrix not found: {video_matrix_path}")# Combine extracted featuresdef combine_features(root_dir, video_range, output_file):    combined_features = []    for video_index in video_range:        feature_file = os.path.join(root_dir, f"vid_{video_index}_features", f"video_{video_index}_features.npy")        if os.path.exists(feature_file):            features = np.load(feature_file)            combined_features.append(features)        else:            print(f"Feature file not found: {feature_file}")    if combined_features:        combined_features = np.stack(combined_features, axis=0)        np.save(output_file, combined_features)        print(f"Saved combined features to {output_file}")# if __name__ == "__main__":    # Extract motion vectors in Linux at first.    # root_dir = r"E:\dataset\dash_video\motion_video"    # output_root_dir = r"E:\dataset\dash_video\motion_video_features"    # video_range = range(1,44)    # save_feature_map()    # save_combined_video_matrix()    # model = CNNSubNetwork()    # batch_extract_features(root_dir, video_range, model, output_root_dir)    # root_dir = r"E:\dataset\dash_video\motion_video_features"    # output_path = r"E:\dataset\dash_video\combined_features.npy"    # combine_features(root_dir, output_path)    # aa = np.load(output_path)    # print(aa.shape)